/home/wyh21/anaconda3/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2024-03-26 10:50:15.496728: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-26 10:50:15.544012: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-26 10:50:16.741755: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/wyh21/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/wyh21/anaconda3 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/wyh21/jdk-11.0.16/jre')}
  warn(msg)
/home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/freesurfer/local')}
  warn(msg)
/home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/wyh21/jdk-11.0.16/lib/tools.jar'), PosixPath('/home/wyh21/jdk-11.0.16/lib/dt.jar')}
  warn(msg)
/home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda-11.6/lib64/libcudart.so'), PosixPath('/usr/local/cuda-11.6/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.
Either way, this might cause trouble in the future:
If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.
  warn(msg)
Global seed set to 42
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda116.so
CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.6/lib64/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 116
CUDA SETUP: Loading binary /home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda116.so...
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/data/wyh21/iu_xray/annotation.json',
 'base_dir': '/data/wyh21/iu_xray/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'iu-xray',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'embed_dim': 512,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': None,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 1.0,
 'llama_model': 'mistralai/Mistral-7B-Instruct-v0.2',
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 20,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'pretrain_batch': 512,
 'pretrain_checkpoint_path': '/home/wyh21/report_generation/Hint_R2GenGPT/pretrain/iu-xray/v1/checkpoints/last.ckpt',
 'pretrain_image_encoder_lr': 1,
 'pretrain_lr': 1,
 'pretrain_max_epochs': 50,
 'pretrain_path': 'pretrain/iu-xray/v1',
 'pretrain_text_encoder_lr': 1,
 'pretraining': False,
 'repetition_penalty': 2.0,
 'savedmodel_path': '/data/wyh21/Hint_R2GenGPT/save/mimic/v1',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp_find_unused_parameters_true',
 'temperature': 0,
 'test': False,
 'test_batch_size': 8,
 'text_model': '/data/wyh21/huggingface/models--emilyalsentzer--Bio_ClinicalBERT',
 'val_batch_size': 8,
 'val_check_interval': 1.0,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
3333333
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading LLAMA
Loading LLAMA model:mistralai/Mistral-7B-Instruct-v0.2
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 9900.01it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.05it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:01<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.05it/s]
[rank: 0] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
/home/wyh21/anaconda3/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/wyh21/anaconda3/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/wyh21/anaconda3/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2024-03-26 10:51:25.362845: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-26 10:51:25.409813: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-26 10:51:26.131987: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-26 10:51:26.178702: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-26 10:51:26.532690: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/wyh21/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-03-26 10:51:26.824473: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-26 10:51:26.869158: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/wyh21/anaconda3 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/wyh21/jdk-11.0.16/jre')}
  warn(msg)
/home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/freesurfer/local')}
  warn(msg)
/home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/wyh21/jdk-11.0.16/lib/tools.jar'), PosixPath('/home/wyh21/jdk-11.0.16/lib/dt.jar')}
  warn(msg)
/home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//hf-mirror.com'), PosixPath('https')}
  warn(msg)
/home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda-11.6/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda-11.6/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.
Either way, this might cause trouble in the future:
If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.
  warn(msg)
2024-03-26 10:51:27.491226: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/wyh21/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
[rank: 1] Global seed set to 42
2024-03-26 10:51:28.077449: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/wyh21/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/wyh21/anaconda3 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/wyh21/jdk-11.0.16/jre')}
  warn(msg)
/home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/freesurfer/local')}
  warn(msg)
/home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/wyh21/jdk-11.0.16/lib/dt.jar'), PosixPath('/home/wyh21/jdk-11.0.16/lib/tools.jar')}
  warn(msg)
/home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('https'), PosixPath('//hf-mirror.com')}
  warn(msg)
/home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda-11.6/lib64/libcudart.so'), PosixPath('/usr/local/cuda-11.6/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.
Either way, this might cause trouble in the future:
If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.
  warn(msg)
/home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/wyh21/anaconda3 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/wyh21/jdk-11.0.16/jre')}
  warn(msg)
/home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/freesurfer/local')}
  warn(msg)
/home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/wyh21/jdk-11.0.16/lib/tools.jar'), PosixPath('/home/wyh21/jdk-11.0.16/lib/dt.jar')}
  warn(msg)
/home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('https'), PosixPath('//hf-mirror.com')}
  warn(msg)
/home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda-11.6/lib64/libcudart.so'), PosixPath('/usr/local/cuda-11.6/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.
Either way, this might cause trouble in the future:
If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.
  warn(msg)
[rank: 2] Global seed set to 42
[rank: 3] Global seed set to 42

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda116.so
CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.6/lib64/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 116
CUDA SETUP: Loading binary /home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda116.so...
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/data/wyh21/iu_xray/annotation.json',
 'base_dir': '/data/wyh21/iu_xray/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'iu-xray',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'embed_dim': 512,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': None,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 1.0,
 'llama_model': 'mistralai/Mistral-7B-Instruct-v0.2',
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 20,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'pretrain_batch': 512,
 'pretrain_checkpoint_path': '/home/wyh21/report_generation/Hint_R2GenGPT/pretrain/iu-xray/v1/checkpoints/last.ckpt',
 'pretrain_image_encoder_lr': 1,
 'pretrain_lr': 1,
 'pretrain_max_epochs': 50,
 'pretrain_path': 'pretrain/iu-xray/v1',
 'pretrain_text_encoder_lr': 1,
 'pretraining': False,
 'repetition_penalty': 2.0,
 'savedmodel_path': '/data/wyh21/Hint_R2GenGPT/save/mimic/v1',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp_find_unused_parameters_true',
 'temperature': 0,
 'test': False,
 'test_batch_size': 8,
 'text_model': '/data/wyh21/huggingface/models--emilyalsentzer--Bio_ClinicalBERT',
 'val_batch_size': 8,
 'val_check_interval': 1.0,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
3333333
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading LLAMA
Loading LLAMA model:mistralai/Mistral-7B-Instruct-v0.2
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 13245.17it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda116.so
CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.6/lib64/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 116
CUDA SETUP: Loading binary /home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda116.so...
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/data/wyh21/iu_xray/annotation.json',
 'base_dir': '/data/wyh21/iu_xray/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'iu-xray',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'embed_dim': 512,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': None,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 1.0,
 'llama_model': 'mistralai/Mistral-7B-Instruct-v0.2',
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 20,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'pretrain_batch': 512,
 'pretrain_checkpoint_path': '/home/wyh21/report_generation/Hint_R2GenGPT/pretrain/iu-xray/v1/checkpoints/last.ckpt',
 'pretrain_image_encoder_lr': 1,
 'pretrain_lr': 1,
 'pretrain_max_epochs': 50,
 'pretrain_path': 'pretrain/iu-xray/v1',
 'pretrain_text_encoder_lr': 1,
 'pretraining': False,
 'repetition_penalty': 2.0,
 'savedmodel_path': '/data/wyh21/Hint_R2GenGPT/save/mimic/v1',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp_find_unused_parameters_true',
 'temperature': 0,
 'test': False,
 'test_batch_size': 8,
 'text_model': '/data/wyh21/huggingface/models--emilyalsentzer--Bio_ClinicalBERT',
 'val_batch_size': 8,
 'val_check_interval': 1.0,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
3333333
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading LLAMA
Loading LLAMA model:mistralai/Mistral-7B-Instruct-v0.2
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 12918.80it/s]

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda116.so
CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.6/lib64/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 116
CUDA SETUP: Loading binary /home/wyh21/anaconda3/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda116.so...
{'accelerator': 'gpu',
 'accumulate_grad_batches': 1,
 'annotation': '/data/wyh21/iu_xray/annotation.json',
 'base_dir': '/data/wyh21/iu_xray/images',
 'batch_size': 8,
 'beam_size': 3,
 'ckpt_file': None,
 'dataset': 'iu-xray',
 'delta_file': None,
 'devices': 4,
 'diversity_penalty': 0,
 'do_sample': False,
 'embed_dim': 512,
 'end_sym': '</s>',
 'every_n_train_steps': 0,
 'freeze_vm': True,
 'global_only': False,
 'gradient_clip_val': None,
 'learning_rate': 0.0001,
 'length_penalty': 2.0,
 'limit_test_batches': 1.0,
 'limit_train_batches': 1.0,
 'limit_val_batches': 1.0,
 'llama_model': 'mistralai/Mistral-7B-Instruct-v0.2',
 'llm_alpha': 16,
 'llm_r': 16,
 'llm_use_lora': False,
 'lora_dropout': 0.1,
 'low_resource': False,
 'max_epochs': 20,
 'max_length': 100,
 'max_new_tokens': 120,
 'min_new_tokens': 80,
 'no_repeat_ngram_size': 2,
 'num_beam_groups': 1,
 'num_nodes': 1,
 'num_sanity_val_steps': 2,
 'num_workers': 8,
 'precision': 'bf16-mixed',
 'prefetch_factor': 4,
 'pretrain_batch': 512,
 'pretrain_checkpoint_path': '/home/wyh21/report_generation/Hint_R2GenGPT/pretrain/iu-xray/v1/checkpoints/last.ckpt',
 'pretrain_image_encoder_lr': 1,
 'pretrain_lr': 1,
 'pretrain_max_epochs': 50,
 'pretrain_path': 'pretrain/iu-xray/v1',
 'pretrain_text_encoder_lr': 1,
 'pretraining': False,
 'repetition_penalty': 2.0,
 'savedmodel_path': '/data/wyh21/Hint_R2GenGPT/save/mimic/v1',
 'scorer_types': ['Bleu_4', 'CIDEr'],
 'strategy': 'ddp_find_unused_parameters_true',
 'temperature': 0,
 'test': False,
 'test_batch_size': 8,
 'text_model': '/data/wyh21/huggingface/models--emilyalsentzer--Bio_ClinicalBERT',
 'val_batch_size': 8,
 'val_check_interval': 1.0,
 'validate': False,
 'vis_alpha': 16,
 'vis_r': 16,
 'vis_use_lora': False,
 'vision_model': 'microsoft/swin-base-patch4-window7-224',
 'weights': [0.5, 0.5]}
3333333
Loading Frozen vision encoder:microsoft/swin-base-patch4-window7-224 -- Done
Loading LLAMA
Loading LLAMA model:mistralai/Mistral-7B-Instruct-v0.2
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 9830.40it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.14it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.03s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.14s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.13s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.10s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.07s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.05s/it][rank: 1] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.20s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.03s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.18s/it]
[rank: 3] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
[rank: 2] Global seed set to 42
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.
Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.
Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.
Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.
Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.
Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.
Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.
Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.
Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.
Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.
Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.
Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5]

  | Name             | Type                 | Params
----------------------------------------------------------
0 | clip_model       | CLIPDualEncoderModel | 198 M 
1 | visual_encoder   | SwinModel            | 86.7 M
2 | text_encoder     | BertModel            | 108 M 
3 | image_projection | ProjectionHead       | 2.1 M 
4 | text_projection  | ProjectionHead       | 1.8 M 
5 | llama_model      | MistralForCausalLM   | 7.2 B 
6 | embed_tokens     | Embedding            | 131 M 
7 | llama_proj       | Linear               | 4.2 M 
8 | hint_proj        | Linear               | 1.0 M 
9 | layer_norm       | LayerNorm            | 8.2 K 
----------------------------------------------------------
117 M     Trainable params
7.3 B     Non-trainable params
7.4 B     Total params
29,783.929Total estimated model params size (MB)
Loading LLAMA Done
sample 1 num of prompts for Atelectasis from total 1
sample 1 num of prompts for Cardiomegaly from total 1
sample 1 num of prompts for Consolidation from total 1
sample 1 num of prompts for Edema from total 1
sample 1 num of prompts for Enlarged Cardiomediastinum from total 1
sample 1 num of prompts for Fracture from total 1
sample 1 num of prompts for Lung Lesion from total 1
sample 1 num of prompts for Lung Opacity from total 1
sample 1 num of prompts for No Finding from total 1
sample 1 num of prompts for Pleural Effusion from total 1
sample 1 num of prompts for Pleural Other from total 1
sample 1 num of prompts for Pneumonia from total 1
sample 1 num of prompts for Pneumothorax from total 1
sample 1 num of prompts for Support Devices from total 1
Sanity Checking: 0it [00:00, ?it/s]Loading LLAMA Done
sample 1 num of prompts for Atelectasis from total 1
sample 1 num of prompts for Cardiomegaly from total 1
sample 1 num of prompts for Consolidation from total 1
sample 1 num of prompts for Edema from total 1
sample 1 num of prompts for Enlarged Cardiomediastinum from total 1
sample 1 num of prompts for Fracture from total 1
sample 1 num of prompts for Lung Lesion from total 1
sample 1 num of prompts for Lung Opacity from total 1
sample 1 num of prompts for No Finding from total 1
sample 1 num of prompts for Pleural Effusion from total 1
sample 1 num of prompts for Pleural Other from total 1
sample 1 num of prompts for Pneumonia from total 1
sample 1 num of prompts for Pneumothorax from total 1
sample 1 num of prompts for Support Devices from total 1
Loading LLAMA Done
sample 1 num of prompts for Atelectasis from total 1
sample 1 num of prompts for Cardiomegaly from total 1
sample 1 num of prompts for Consolidation from total 1
sample 1 num of prompts for Edema from total 1
sample 1 num of prompts for Enlarged Cardiomediastinum from total 1
sample 1 num of prompts for Fracture from total 1
sample 1 num of prompts for Lung Lesion from total 1
sample 1 num of prompts for Lung Opacity from total 1
sample 1 num of prompts for No Finding from total 1
sample 1 num of prompts for Pleural Effusion from total 1
sample 1 num of prompts for Pleural Other from total 1
sample 1 num of prompts for Pneumonia from total 1
sample 1 num of prompts for Pneumothorax from total 1
sample 1 num of prompts for Support Devices from total 1
Loading LLAMA Done
sample 1 num of prompts for Atelectasis from total 1
sample 1 num of prompts for Cardiomegaly from total 1
sample 1 num of prompts for Consolidation from total 1
sample 1 num of prompts for Edema from total 1
sample 1 num of prompts for Enlarged Cardiomediastinum from total 1
sample 1 num of prompts for Fracture from total 1
sample 1 num of prompts for Lung Lesion from total 1
sample 1 num of prompts for Lung Opacity from total 1
sample 1 num of prompts for No Finding from total 1
sample 1 num of prompts for Pleural Effusion from total 1
sample 1 num of prompts for Pleural Other from total 1
sample 1 num of prompts for Pneumonia from total 1
sample 1 num of prompts for Pneumothorax from total 1
sample 1 num of prompts for Support Devices from total 1
Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:15<00:15, 15.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:28<00:00, 14.14s/it]{'testlen': 1479, 'reflen': 579, 'guess': [1479, 1463, 1447, 1431], 'correct': [22, 0, 0, 0]}
ratio: 2.5544041450733084
                                                                           {'Bleu_1': 0.014874915483424695, 'Bleu_2': 1.0083355763566252e-10, 'Bleu_3': 1.9153459006895001e-13, 'Bleu_4': 8.370964573240609e-15, 'ROUGE_L': 0.020749925080235265, 'CIDEr': 1.0241663337448323e-10}
Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:28<00:00, 14.31s/it]                                                                           Saving checkpoint at step 0 to /data/wyh21/Hint_R2GenGPT/save/mimic/v1/checkpoints/checkpoint_epoch0_step0_bleu0.000000_cider0.000000.pth.
Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:28<00:00, 14.33s/it]                                                                           {'testlen': 1386, 'reflen': 537, 'guess': [1386, 1370, 1354, 1338], 'correct': [21, 0, 0, 0]}
ratio: 2.5810055865873727
{'testlen': 1459, 'reflen': 556, 'guess': [1459, 1443, 1427, 1411], 'correct': [19, 0, 0, 0]}
ratio: 2.624100719419741
{'testlen': 1465, 'reflen': 618, 'guess': [1465, 1449, 1433, 1417], 'correct': [26, 1, 0, 0]}
ratio: 2.370550161808462
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable 	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISMTOKENIZERS_PARALLELISM=(true | false)
=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/64 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/64 [00:00<?, ?it/s] huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable 	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISMTOKENIZERS_PARALLELISM=(true | false)
=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable 	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Epoch 0:   2%|▏         | 1/64 [00:03<03:44,  3.56s/it]Epoch 0:   2%|▏         | 1/64 [00:03<03:45,  3.57s/it, v_num=1, loss=9.810]Epoch 0:   3%|▎         | 2/64 [00:04<02:25,  2.34s/it, v_num=1, loss=9.810]Epoch 0:   3%|▎         | 2/64 [00:04<02:25,  2.35s/it, v_num=1, loss=11.40]Epoch 0:   5%|▍         | 3/64 [00:05<01:57,  1.93s/it, v_num=1, loss=11.40]Epoch 0:   5%|▍         | 3/64 [00:05<01:58,  1.94s/it, v_num=1, loss=7.000]Epoch 0:   6%|▋         | 4/64 [00:06<01:44,  1.74s/it, v_num=1, loss=7.000]Epoch 0:   6%|▋         | 4/64 [00:06<01:44,  1.74s/it, v_num=1, loss=5.430]Epoch 0:   8%|▊         | 5/64 [00:08<01:35,  1.61s/it, v_num=1, loss=5.430]Epoch 0:   8%|▊         | 5/64 [00:08<01:35,  1.61s/it, v_num=1, loss=4.450]Epoch 0:   9%|▉         | 6/64 [00:09<01:28,  1.53s/it, v_num=1, loss=4.450]Epoch 0:   9%|▉         | 6/64 [00:09<01:28,  1.53s/it, v_num=1, loss=3.670]Epoch 0:  11%|█         | 7/64 [00:10<01:23,  1.47s/it, v_num=1, loss=3.670]Epoch 0:  11%|█         | 7/64 [00:10<01:23,  1.47s/it, v_num=1, loss=3.210]Epoch 0:  12%|█▎        | 8/64 [00:11<01:20,  1.43s/it, v_num=1, loss=3.210]Epoch 0:  12%|█▎        | 8/64 [00:11<01:20,  1.43s/it, v_num=1, loss=3.190]Epoch 0:  14%|█▍        | 9/64 [00:12<01:16,  1.40s/it, v_num=1, loss=3.190]Epoch 0:  14%|█▍        | 9/64 [00:12<01:17,  1.40s/it, v_num=1, loss=2.710]Epoch 0:  16%|█▌        | 10/64 [00:13<01:14,  1.37s/it, v_num=1, loss=2.710]Epoch 0:  16%|█▌        | 10/64 [00:13<01:14,  1.37s/it, v_num=1, loss=2.840]Epoch 0:  17%|█▋        | 11/64 [00:14<01:11,  1.35s/it, v_num=1, loss=2.840]Epoch 0:  17%|█▋        | 11/64 [00:14<01:11,  1.35s/it, v_num=1, loss=2.720]Epoch 0:  19%|█▉        | 12/64 [00:15<01:09,  1.33s/it, v_num=1, loss=2.720]Epoch 0:  19%|█▉        | 12/64 [00:16<01:09,  1.33s/it, v_num=1, loss=2.460]Epoch 0:  20%|██        | 13/64 [00:17<01:07,  1.32s/it, v_num=1, loss=2.460]Epoch 0:  20%|██        | 13/64 [00:17<01:07,  1.32s/it, v_num=1, loss=2.220]Epoch 0:  22%|██▏       | 14/64 [00:18<01:05,  1.31s/it, v_num=1, loss=2.220]Epoch 0:  22%|██▏       | 14/64 [00:18<01:05,  1.31s/it, v_num=1, loss=2.380]Epoch 0:  23%|██▎       | 15/64 [00:19<01:03,  1.29s/it, v_num=1, loss=2.380]Epoch 0:  23%|██▎       | 15/64 [00:19<01:03,  1.30s/it, v_num=1, loss=1.950]Epoch 0:  25%|██▌       | 16/64 [00:20<01:01,  1.28s/it, v_num=1, loss=1.950]Epoch 0:  25%|██▌       | 16/64 [00:20<01:01,  1.28s/it, v_num=1, loss=2.050]Epoch 0:  27%|██▋       | 17/64 [00:21<00:59,  1.28s/it, v_num=1, loss=2.050]Epoch 0:  27%|██▋       | 17/64 [00:21<01:00,  1.28s/it, v_num=1, loss=2.210]Epoch 0:  28%|██▊       | 18/64 [00:22<00:58,  1.27s/it, v_num=1, loss=2.210]Epoch 0:  28%|██▊       | 18/64 [00:22<00:58,  1.27s/it, v_num=1, loss=1.860]Epoch 0:  30%|██▉       | 19/64 [00:23<00:56,  1.26s/it, v_num=1, loss=1.860]Epoch 0:  30%|██▉       | 19/64 [00:23<00:56,  1.26s/it, v_num=1, loss=2.210]Epoch 0:  31%|███▏      | 20/64 [00:25<00:55,  1.26s/it, v_num=1, loss=2.210]Epoch 0:  31%|███▏      | 20/64 [00:25<00:55,  1.26s/it, v_num=1, loss=2.500]Epoch 0:  33%|███▎      | 21/64 [00:26<00:53,  1.25s/it, v_num=1, loss=2.500]Epoch 0:  33%|███▎      | 21/64 [00:26<00:53,  1.25s/it, v_num=1, loss=2.100]Epoch 0:  34%|███▍      | 22/64 [00:27<00:52,  1.25s/it, v_num=1, loss=2.100]Epoch 0:  34%|███▍      | 22/64 [00:27<00:52,  1.25s/it, v_num=1, loss=2.460]Epoch 0:  36%|███▌      | 23/64 [00:28<00:51,  1.24s/it, v_num=1, loss=2.460]Epoch 0:  36%|███▌      | 23/64 [00:28<00:51,  1.24s/it, v_num=1, loss=2.050]Epoch 0:  38%|███▊      | 24/64 [00:29<00:49,  1.24s/it, v_num=1, loss=2.050]Epoch 0:  38%|███▊      | 24/64 [00:29<00:49,  1.24s/it, v_num=1, loss=1.950]Epoch 0:  39%|███▉      | 25/64 [00:30<00:48,  1.24s/it, v_num=1, loss=1.950]Epoch 0:  39%|███▉      | 25/64 [00:30<00:48,  1.24s/it, v_num=1, loss=1.920]Epoch 0:  41%|████      | 26/64 [00:32<00:46,  1.23s/it, v_num=1, loss=1.920]Epoch 0:  41%|████      | 26/64 [00:32<00:46,  1.23s/it, v_num=1, loss=1.910]Epoch 0:  42%|████▏     | 27/64 [00:33<00:45,  1.23s/it, v_num=1, loss=1.910]Epoch 0:  42%|████▏     | 27/64 [00:33<00:45,  1.23s/it, v_num=1, loss=1.800]Epoch 0:  44%|████▍     | 28/64 [00:34<00:44,  1.23s/it, v_num=1, loss=1.800]Epoch 0:  44%|████▍     | 28/64 [00:34<00:44,  1.23s/it, v_num=1, loss=2.090]Epoch 0:  45%|████▌     | 29/64 [00:35<00:42,  1.22s/it, v_num=1, loss=2.090]Epoch 0:  45%|████▌     | 29/64 [00:35<00:42,  1.22s/it, v_num=1, loss=2.170]Epoch 0:  47%|████▋     | 30/64 [00:36<00:41,  1.22s/it, v_num=1, loss=2.170]Epoch 0:  47%|████▋     | 30/64 [00:36<00:41,  1.22s/it, v_num=1, loss=2.040]Epoch 0:  48%|████▊     | 31/64 [00:37<00:40,  1.22s/it, v_num=1, loss=2.040]Epoch 0:  48%|████▊     | 31/64 [00:37<00:40,  1.22s/it, v_num=1, loss=1.950]Epoch 0:  50%|█████     | 32/64 [00:38<00:38,  1.22s/it, v_num=1, loss=1.950]Epoch 0:  50%|█████     | 32/64 [00:38<00:38,  1.22s/it, v_num=1, loss=2.010]Epoch 0:  52%|█████▏    | 33/64 [00:40<00:37,  1.21s/it, v_num=1, loss=2.010]Epoch 0:  52%|█████▏    | 33/64 [00:40<00:37,  1.21s/it, v_num=1, loss=1.900]Epoch 0:  53%|█████▎    | 34/64 [00:41<00:36,  1.21s/it, v_num=1, loss=1.900]Epoch 0:  53%|█████▎    | 34/64 [00:41<00:36,  1.21s/it, v_num=1, loss=1.560]Epoch 0:  55%|█████▍    | 35/64 [00:42<00:35,  1.21s/it, v_num=1, loss=1.560]Epoch 0:  55%|█████▍    | 35/64 [00:42<00:35,  1.21s/it, v_num=1, loss=1.690]Epoch 0:  56%|█████▋    | 36/64 [00:43<00:33,  1.21s/it, v_num=1, loss=1.690]Epoch 0:  56%|█████▋    | 36/64 [00:43<00:33,  1.21s/it, v_num=1, loss=1.960]Epoch 0:  58%|█████▊    | 37/64 [00:44<00:32,  1.21s/it, v_num=1, loss=1.960]Epoch 0:  58%|█████▊    | 37/64 [00:44<00:32,  1.21s/it, v_num=1, loss=2.070]Epoch 0:  59%|█████▉    | 38/64 [00:45<00:31,  1.20s/it, v_num=1, loss=2.070]Epoch 0:  59%|█████▉    | 38/64 [00:45<00:31,  1.20s/it, v_num=1, loss=1.630]Epoch 0:  61%|██████    | 39/64 [00:46<00:30,  1.20s/it, v_num=1, loss=1.630]Epoch 0:  61%|██████    | 39/64 [00:46<00:30,  1.20s/it, v_num=1, loss=1.940]Epoch 0:  62%|██████▎   | 40/64 [00:48<00:28,  1.20s/it, v_num=1, loss=1.940]Epoch 0:  62%|██████▎   | 40/64 [00:48<00:28,  1.20s/it, v_num=1, loss=2.100]Epoch 0:  64%|██████▍   | 41/64 [00:49<00:27,  1.20s/it, v_num=1, loss=2.100]Epoch 0:  64%|██████▍   | 41/64 [00:49<00:27,  1.20s/it, v_num=1, loss=2.150]Epoch 0:  66%|██████▌   | 42/64 [00:50<00:26,  1.20s/it, v_num=1, loss=2.150]Epoch 0:  66%|██████▌   | 42/64 [00:50<00:26,  1.20s/it, v_num=1, loss=1.970]Epoch 0:  67%|██████▋   | 43/64 [00:51<00:25,  1.20s/it, v_num=1, loss=1.970]Epoch 0:  67%|██████▋   | 43/64 [00:51<00:25,  1.20s/it, v_num=1, loss=1.720]Epoch 0:  69%|██████▉   | 44/64 [00:52<00:23,  1.20s/it, v_num=1, loss=1.720]Epoch 0:  69%|██████▉   | 44/64 [00:52<00:23,  1.20s/it, v_num=1, loss=1.980]Epoch 0:  70%|███████   | 45/64 [00:53<00:22,  1.19s/it, v_num=1, loss=1.980]Epoch 0:  70%|███████   | 45/64 [00:53<00:22,  1.19s/it, v_num=1, loss=1.610]Epoch 0:  72%|███████▏  | 46/64 [00:54<00:21,  1.19s/it, v_num=1, loss=1.610]Epoch 0:  72%|███████▏  | 46/64 [00:54<00:21,  1.19s/it, v_num=1, loss=1.820]Epoch 0:  73%|███████▎  | 47/64 [00:56<00:20,  1.19s/it, v_num=1, loss=1.820]Epoch 0:  73%|███████▎  | 47/64 [00:56<00:20,  1.19s/it, v_num=1, loss=1.890]Epoch 0:  75%|███████▌  | 48/64 [00:57<00:19,  1.19s/it, v_num=1, loss=1.890]Epoch 0:  75%|███████▌  | 48/64 [00:57<00:19,  1.19s/it, v_num=1, loss=1.560]Epoch 0:  77%|███████▋  | 49/64 [00:58<00:17,  1.19s/it, v_num=1, loss=1.560]Epoch 0:  77%|███████▋  | 49/64 [00:58<00:17,  1.19s/it, v_num=1, loss=1.530]Epoch 0:  78%|███████▊  | 50/64 [00:59<00:16,  1.19s/it, v_num=1, loss=1.530]Epoch 0:  78%|███████▊  | 50/64 [00:59<00:16,  1.19s/it, v_num=1, loss=1.760]Epoch 0:  80%|███████▉  | 51/64 [01:00<00:15,  1.19s/it, v_num=1, loss=1.760]Epoch 0:  80%|███████▉  | 51/64 [01:00<00:15,  1.19s/it, v_num=1, loss=1.870]Epoch 0:  81%|████████▏ | 52/64 [01:01<00:14,  1.19s/it, v_num=1, loss=1.870]Epoch 0:  81%|████████▏ | 52/64 [01:01<00:14,  1.19s/it, v_num=1, loss=1.940]Epoch 0:  83%|████████▎ | 53/64 [01:03<00:13,  1.19s/it, v_num=1, loss=1.940]Epoch 0:  83%|████████▎ | 53/64 [01:03<00:13,  1.19s/it, v_num=1, loss=1.860]Epoch 0:  84%|████████▍ | 54/64 [01:04<00:11,  1.19s/it, v_num=1, loss=1.860]Epoch 0:  84%|████████▍ | 54/64 [01:04<00:11,  1.19s/it, v_num=1, loss=1.870]Epoch 0:  86%|████████▌ | 55/64 [01:05<00:10,  1.19s/it, v_num=1, loss=1.870]Epoch 0:  86%|████████▌ | 55/64 [01:05<00:10,  1.19s/it, v_num=1, loss=1.710]Epoch 0:  88%|████████▊ | 56/64 [01:06<00:09,  1.19s/it, v_num=1, loss=1.710]Epoch 0:  88%|████████▊ | 56/64 [01:06<00:09,  1.19s/it, v_num=1, loss=1.330]Epoch 0:  89%|████████▉ | 57/64 [01:07<00:08,  1.19s/it, v_num=1, loss=1.330]Epoch 0:  89%|████████▉ | 57/64 [01:07<00:08,  1.19s/it, v_num=1, loss=1.500]Epoch 0:  91%|█████████ | 58/64 [01:09<00:07,  1.19s/it, v_num=1, loss=1.500]Epoch 0:  91%|█████████ | 58/64 [01:09<00:07,  1.19s/it, v_num=1, loss=1.980]Epoch 0:  92%|█████████▏| 59/64 [01:10<00:05,  1.19s/it, v_num=1, loss=1.980]Epoch 0:  92%|█████████▏| 59/64 [01:10<00:05,  1.19s/it, v_num=1, loss=1.760]Epoch 0:  94%|█████████▍| 60/64 [01:11<00:04,  1.19s/it, v_num=1, loss=1.760]Epoch 0:  94%|█████████▍| 60/64 [01:11<00:04,  1.19s/it, v_num=1, loss=1.530]Epoch 0:  95%|█████████▌| 61/64 [01:12<00:03,  1.19s/it, v_num=1, loss=1.530]Epoch 0:  95%|█████████▌| 61/64 [01:12<00:03,  1.19s/it, v_num=1, loss=1.850]Epoch 0:  97%|█████████▋| 62/64 [01:13<00:02,  1.19s/it, v_num=1, loss=1.850]Epoch 0:  97%|█████████▋| 62/64 [01:13<00:02,  1.19s/it, v_num=1, loss=1.710]Epoch 0:  98%|█████████▊| 63/64 [01:15<00:01,  1.19s/it, v_num=1, loss=1.710]Epoch 0:  98%|█████████▊| 63/64 [01:15<00:01,  1.19s/it, v_num=1, loss=2.080]Epoch 0: 100%|██████████| 64/64 [01:16<00:00,  1.19s/it, v_num=1, loss=2.080]Epoch 0: 100%|██████████| 64/64 [01:16<00:00,  1.19s/it, v_num=1, loss=2.170]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: 0it [00:00, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][AThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Validation DataLoader 0:  10%|█         | 1/10 [00:13<01:59, 13.28s/it][AThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Validation DataLoader 0:  20%|██        | 2/10 [00:26<01:47, 13.40s/it][AThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Validation DataLoader 0:  30%|███       | 3/10 [00:40<01:34, 13.56s/it][AThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Validation DataLoader 0:  40%|████      | 4/10 [00:54<01:22, 13.70s/it][AThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Validation DataLoader 0:  50%|█████     | 5/10 [01:09<01:09, 13.91s/it][AThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Validation DataLoader 0:  60%|██████    | 6/10 [01:24<00:56, 14.06s/it][AThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Validation DataLoader 0:  70%|███████   | 7/10 [01:39<00:42, 14.26s/it][AThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Validation DataLoader 0:  80%|████████  | 8/10 [01:55<00:28, 14.49s/it][AThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Validation DataLoader 0:  90%|█████████ | 9/10 [02:12<00:14, 14.67s/it][AThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Validation DataLoader 0: 100%|██████████| 10/10 [02:27<00:00, 14.76s/it][A{'testlen': 5236, 'reflen': 2621, 'guess': [5236, 5162, 5088, 5014], 'correct': [1483, 579, 273, 143]}
ratio: 1.9977107974048083
                                                                             
                                                                        [A{'Bleu_1': 0.28323147440789087, 'Bleu_2': 0.17823830288599415, 'Bleu_3': 0.1194554238777194, 'Bleu_4': 0.08350112605957188, 'ROUGE_L': 0.2821278065605744, 'CIDEr': 0.009430468562788535}
Epoch 0: 100%|██████████| 64/64 [03:45<00:00,  3.52s/it, v_num=1, loss=2.170]
Validation DataLoader 0: 100%|██████████| 10/10 [02:27<00:00, 14.77s/it][A                                                                             
                                                                        [ASaving checkpoint at step 64 to /data/wyh21/Hint_R2GenGPT/save/mimic/v1/checkpoints/checkpoint_epoch0_step64_bleu0.083501_cider0.009430.pth.
Epoch 0: 100%|██████████| 64/64 [03:45<00:00,  3.52s/it, v_num=1, loss=2.170]
Validation DataLoader 0: 100%|██████████| 10/10 [02:27<00:00, 14.78s/it][AThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Epoch 0: 100%|██████████| 64/64 [05:01<00:00,  4.71s/it, v_num=1, loss=2.170]
                                                                        [AEpoch 0: 100%|██████████| 64/64 [05:01<00:00,  4.71s/it, v_num=1, loss=2.170]{'testlen': 5060, 'reflen': 2532, 'guess': [5060, 4986, 4912, 4838], 'correct': [1373, 508, 226, 107]}
ratio: 1.9984202211682471
{'testlen': 4998, 'reflen': 2557, 'guess': [4998, 4924, 4850, 4776], 'correct': [1453, 562, 258, 124]}
ratio: 1.9546343371130408
{'testlen': 5161, 'reflen': 2684, 'guess': [5161, 5087, 5013, 4939], 'correct': [1480, 576, 258, 121]}
ratio: 1.9228763040231287
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Epoch 0:   0%|          | 0/64 [00:00<?, ?it/s, v_num=1, loss=2.170]         Epoch 1:   0%|          | 0/64 [00:00<?, ?it/s, v_num=1, loss=2.170]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Epoch 1:   2%|▏         | 1/64 [00:02<02:28,  2.35s/it, v_num=1, loss=2.170]Epoch 1:   2%|▏         | 1/64 [00:02<02:28,  2.36s/it, v_num=1, loss=1.580]Epoch 1:   3%|▎         | 2/64 [00:03<01:48,  1.75s/it, v_num=1, loss=1.580]Epoch 1:   3%|▎         | 2/64 [00:03<01:49,  1.76s/it, v_num=1, loss=1.840]Epoch 1:   5%|▍         | 3/64 [00:05<01:52,  1.85s/it, v_num=1, loss=1.840]Epoch 1:   5%|▍         | 3/64 [00:05<01:52,  1.85s/it, v_num=1, loss=1.730]Epoch 1:   6%|▋         | 4/64 [00:06<01:44,  1.74s/it, v_num=1, loss=1.730]Epoch 1:   6%|▋         | 4/64 [00:06<01:44,  1.74s/it, v_num=1, loss=1.680]Epoch 1:   8%|▊         | 5/64 [00:08<01:40,  1.70s/it, v_num=1, loss=1.680]Epoch 1:   8%|▊         | 5/64 [00:08<01:40,  1.70s/it, v_num=1, loss=1.710]Epoch 1:   9%|▉         | 6/64 [00:10<01:37,  1.67s/it, v_num=1, loss=1.710]Epoch 1:   9%|▉         | 6/64 [00:10<01:37,  1.67s/it, v_num=1, loss=1.880]Epoch 1:  11%|█         | 7/64 [00:11<01:33,  1.64s/it, v_num=1, loss=1.880]Epoch 1:  11%|█         | 7/64 [00:11<01:33,  1.64s/it, v_num=1, loss=1.880]Epoch 1:  12%|█▎        | 8/64 [00:12<01:28,  1.58s/it, v_num=1, loss=1.880]Epoch 1:  12%|█▎        | 8/64 [00:12<01:28,  1.58s/it, v_num=1, loss=1.710]Epoch 1:  14%|█▍        | 9/64 [00:14<01:29,  1.63s/it, v_num=1, loss=1.710]Epoch 1:  14%|█▍        | 9/64 [00:14<01:29,  1.63s/it, v_num=1, loss=1.410]Epoch 1:  16%|█▌        | 10/64 [00:16<01:28,  1.65s/it, v_num=1, loss=1.410]Epoch 1:  16%|█▌        | 10/64 [00:16<01:28,  1.65s/it, v_num=1, loss=1.500]Epoch 1:  17%|█▋        | 11/64 [00:18<01:27,  1.65s/it, v_num=1, loss=1.500]Epoch 1:  17%|█▋        | 11/64 [00:18<01:27,  1.65s/it, v_num=1, loss=1.460]Epoch 1:  19%|█▉        | 12/64 [00:19<01:25,  1.65s/it, v_num=1, loss=1.460]Epoch 1:  19%|█▉        | 12/64 [00:19<01:25,  1.65s/it, v_num=1, loss=1.300]Epoch 1:  20%|██        | 13/64 [00:21<01:23,  1.64s/it, v_num=1, loss=1.300]Epoch 1:  20%|██        | 13/64 [00:21<01:23,  1.64s/it, v_num=1, loss=1.640]Epoch 1:  22%|██▏       | 14/64 [00:23<01:22,  1.65s/it, v_num=1, loss=1.640]Epoch 1:  22%|██▏       | 14/64 [00:23<01:22,  1.65s/it, v_num=1, loss=1.430]Epoch 1:  23%|██▎       | 15/64 [00:24<01:20,  1.65s/it, v_num=1, loss=1.430]Epoch 1:  23%|██▎       | 15/64 [00:24<01:20,  1.65s/it, v_num=1, loss=1.500]Epoch 1:  25%|██▌       | 16/64 [00:26<01:19,  1.65s/it, v_num=1, loss=1.500]Epoch 1:  25%|██▌       | 16/64 [00:26<01:19,  1.65s/it, v_num=1, loss=1.390]Epoch 1:  27%|██▋       | 17/64 [00:28<01:17,  1.65s/it, v_num=1, loss=1.390]Epoch 1:  27%|██▋       | 17/64 [00:28<01:17,  1.65s/it, v_num=1, loss=1.680]Epoch 1:  28%|██▊       | 18/64 [00:29<01:16,  1.66s/it, v_num=1, loss=1.680]Epoch 1:  28%|██▊       | 18/64 [00:29<01:16,  1.66s/it, v_num=1, loss=1.650]Epoch 1:  30%|██▉       | 19/64 [00:31<01:14,  1.66s/it, v_num=1, loss=1.650]Epoch 1:  30%|██▉       | 19/64 [00:31<01:14,  1.66s/it, v_num=1, loss=1.610]Epoch 1:  31%|███▏      | 20/64 [00:33<01:13,  1.67s/it, v_num=1, loss=1.610]Epoch 1:  31%|███▏      | 20/64 [00:33<01:13,  1.67s/it, v_num=1, loss=1.270]Epoch 1:  33%|███▎      | 21/64 [00:35<01:11,  1.67s/it, v_num=1, loss=1.270]Epoch 1:  33%|███▎      | 21/64 [00:35<01:11,  1.67s/it, v_num=1, loss=1.500]Epoch 1:  34%|███▍      | 22/64 [00:36<01:10,  1.67s/it, v_num=1, loss=1.500]Epoch 1:  34%|███▍      | 22/64 [00:36<01:10,  1.67s/it, v_num=1, loss=1.440]Epoch 1:  36%|███▌      | 23/64 [00:38<01:08,  1.66s/it, v_num=1, loss=1.440]Epoch 1:  36%|███▌      | 23/64 [00:38<01:08,  1.66s/it, v_num=1, loss=1.660]Epoch 1:  38%|███▊      | 24/64 [00:40<01:06,  1.67s/it, v_num=1, loss=1.660]Epoch 1:  38%|███▊      | 24/64 [00:40<01:06,  1.67s/it, v_num=1, loss=1.340]Epoch 1:  39%|███▉      | 25/64 [00:41<01:05,  1.67s/it, v_num=1, loss=1.340]Epoch 1:  39%|███▉      | 25/64 [00:41<01:05,  1.67s/it, v_num=1, loss=1.400]Epoch 1:  41%|████      | 26/64 [00:43<01:03,  1.68s/it, v_num=1, loss=1.400]Epoch 1:  41%|████      | 26/64 [00:43<01:03,  1.68s/it, v_num=1, loss=1.390]Epoch 1:  42%|████▏     | 27/64 [00:45<01:01,  1.67s/it, v_num=1, loss=1.390]Epoch 1:  42%|████▏     | 27/64 [00:45<01:01,  1.67s/it, v_num=1, loss=1.130]Epoch 1:  44%|████▍     | 28/64 [00:47<01:01,  1.70s/it, v_num=1, loss=1.130]Epoch 1:  44%|████▍     | 28/64 [00:47<01:01,  1.70s/it, v_num=1, loss=1.690]Epoch 1:  45%|████▌     | 29/64 [00:49<00:59,  1.70s/it, v_num=1, loss=1.690]Epoch 1:  45%|████▌     | 29/64 [00:49<00:59,  1.70s/it, v_num=1, loss=1.500]Epoch 1:  47%|████▋     | 30/64 [00:51<00:57,  1.70s/it, v_num=1, loss=1.500]Epoch 1:  47%|████▋     | 30/64 [00:51<00:57,  1.70s/it, v_num=1, loss=1.520]Epoch 1:  48%|████▊     | 31/64 [00:52<00:56,  1.71s/it, v_num=1, loss=1.520]Epoch 1:  48%|████▊     | 31/64 [00:52<00:56,  1.71s/it, v_num=1, loss=1.410]Epoch 1:  50%|█████     | 32/64 [00:54<00:54,  1.71s/it, v_num=1, loss=1.410]Epoch 1:  50%|█████     | 32/64 [00:54<00:54,  1.71s/it, v_num=1, loss=1.520]Epoch 1:  52%|█████▏    | 33/64 [00:56<00:53,  1.71s/it, v_num=1, loss=1.520]Epoch 1:  52%|█████▏    | 33/64 [00:56<00:53,  1.71s/it, v_num=1, loss=1.410]Epoch 1:  53%|█████▎    | 34/64 [00:58<00:51,  1.72s/it, v_num=1, loss=1.410]Epoch 1:  53%|█████▎    | 34/64 [00:58<00:51,  1.72s/it, v_num=1, loss=1.420]Epoch 1:  55%|█████▍    | 35/64 [00:59<00:49,  1.70s/it, v_num=1, loss=1.420]Epoch 1:  55%|█████▍    | 35/64 [00:59<00:49,  1.70s/it, v_num=1, loss=1.250]Epoch 1:  56%|█████▋    | 36/64 [01:02<00:48,  1.73s/it, v_num=1, loss=1.250]Epoch 1:  56%|█████▋    | 36/64 [01:02<00:48,  1.73s/it, v_num=1, loss=1.350]Epoch 1:  58%|█████▊    | 37/64 [01:03<00:46,  1.72s/it, v_num=1, loss=1.350]Epoch 1:  58%|█████▊    | 37/64 [01:03<00:46,  1.73s/it, v_num=1, loss=1.500]Epoch 1:  59%|█████▉    | 38/64 [01:05<00:44,  1.72s/it, v_num=1, loss=1.500]Epoch 1:  59%|█████▉    | 38/64 [01:05<00:44,  1.72s/it, v_num=1, loss=1.470]Epoch 1:  61%|██████    | 39/64 [01:07<00:43,  1.73s/it, v_num=1, loss=1.470]Epoch 1:  61%|██████    | 39/64 [01:07<00:43,  1.73s/it, v_num=1, loss=1.820]Epoch 1:  62%|██████▎   | 40/64 [01:09<00:41,  1.73s/it, v_num=1, loss=1.820]Epoch 1:  62%|██████▎   | 40/64 [01:09<00:41,  1.73s/it, v_num=1, loss=1.400]Epoch 1:  64%|██████▍   | 41/64 [01:11<00:39,  1.73s/it, v_num=1, loss=1.400]Epoch 1:  64%|██████▍   | 41/64 [01:11<00:39,  1.73s/it, v_num=1, loss=1.580]Epoch 1:  66%|██████▌   | 42/64 [01:12<00:38,  1.74s/it, v_num=1, loss=1.580]Epoch 1:  66%|██████▌   | 42/64 [01:12<00:38,  1.74s/it, v_num=1, loss=1.500]Epoch 1:  67%|██████▋   | 43/64 [01:14<00:36,  1.74s/it, v_num=1, loss=1.500]Epoch 1:  67%|██████▋   | 43/64 [01:14<00:36,  1.74s/it, v_num=1, loss=1.150]Epoch 1:  69%|██████▉   | 44/64 [01:16<00:34,  1.74s/it, v_num=1, loss=1.150]Epoch 1:  69%|██████▉   | 44/64 [01:16<00:34,  1.74s/it, v_num=1, loss=1.590]Epoch 1:  70%|███████   | 45/64 [01:18<00:33,  1.74s/it, v_num=1, loss=1.590]Epoch 1:  70%|███████   | 45/64 [01:18<00:33,  1.74s/it, v_num=1, loss=1.210]Epoch 1:  72%|███████▏  | 46/64 [01:19<00:31,  1.74s/it, v_num=1, loss=1.210]Epoch 1:  72%|███████▏  | 46/64 [01:19<00:31,  1.74s/it, v_num=1, loss=1.750]Epoch 1:  73%|███████▎  | 47/64 [01:21<00:29,  1.74s/it, v_num=1, loss=1.750]Epoch 1:  73%|███████▎  | 47/64 [01:21<00:29,  1.74s/it, v_num=1, loss=1.520]Epoch 1:  75%|███████▌  | 48/64 [01:23<00:27,  1.74s/it, v_num=1, loss=1.520]Epoch 1:  75%|███████▌  | 48/64 [01:23<00:27,  1.74s/it, v_num=1, loss=1.750]Epoch 1:  77%|███████▋  | 49/64 [01:25<00:26,  1.74s/it, v_num=1, loss=1.750]Epoch 1:  77%|███████▋  | 49/64 [01:25<00:26,  1.74s/it, v_num=1, loss=1.640]Epoch 1:  78%|███████▊  | 50/64 [01:26<00:24,  1.74s/it, v_num=1, loss=1.640]Epoch 1:  78%|███████▊  | 50/64 [01:26<00:24,  1.74s/it, v_num=1, loss=1.520]Epoch 1:  80%|███████▉  | 51/64 [01:28<00:22,  1.74s/it, v_num=1, loss=1.520]Epoch 1:  80%|███████▉  | 51/64 [01:28<00:22,  1.74s/it, v_num=1, loss=1.330]Epoch 1:  81%|████████▏ | 52/64 [01:30<00:20,  1.74s/it, v_num=1, loss=1.330]Epoch 1:  81%|████████▏ | 52/64 [01:30<00:20,  1.74s/it, v_num=1, loss=1.390]Epoch 1:  83%|████████▎ | 53/64 [01:32<00:19,  1.74s/it, v_num=1, loss=1.390]Epoch 1:  83%|████████▎ | 53/64 [01:32<00:19,  1.74s/it, v_num=1, loss=1.830]Epoch 1:  84%|████████▍ | 54/64 [01:34<00:17,  1.74s/it, v_num=1, loss=1.830]Epoch 1:  84%|████████▍ | 54/64 [01:34<00:17,  1.74s/it, v_num=1, loss=1.670]Epoch 1:  86%|████████▌ | 55/64 [01:35<00:15,  1.73s/it, v_num=1, loss=1.670]Epoch 1:  86%|████████▌ | 55/64 [01:35<00:15,  1.73s/it, v_num=1, loss=1.390]Epoch 1:  88%|████████▊ | 56/64 [01:37<00:13,  1.74s/it, v_num=1, loss=1.390]Epoch 1:  88%|████████▊ | 56/64 [01:37<00:13,  1.74s/it, v_num=1, loss=1.450]Epoch 1:  89%|████████▉ | 57/64 [01:39<00:12,  1.74s/it, v_num=1, loss=1.450]Epoch 1:  89%|████████▉ | 57/64 [01:39<00:12,  1.74s/it, v_num=1, loss=1.480]Epoch 1:  91%|█████████ | 58/64 [01:41<00:10,  1.75s/it, v_num=1, loss=1.480]Epoch 1:  91%|█████████ | 58/64 [01:41<00:10,  1.75s/it, v_num=1, loss=1.630]Epoch 1:  92%|█████████▏| 59/64 [01:43<00:08,  1.75s/it, v_num=1, loss=1.630]Epoch 1:  92%|█████████▏| 59/64 [01:43<00:08,  1.75s/it, v_num=1, loss=1.220]Epoch 1:  94%|█████████▍| 60/64 [01:44<00:06,  1.74s/it, v_num=1, loss=1.220]Epoch 1:  94%|█████████▍| 60/64 [01:44<00:06,  1.74s/it, v_num=1, loss=1.530]Epoch 1:  95%|█████████▌| 61/64 [01:46<00:05,  1.74s/it, v_num=1, loss=1.530]Epoch 1:  95%|█████████▌| 61/64 [01:46<00:05,  1.74s/it, v_num=1, loss=1.650]Epoch 1:  97%|█████████▋| 62/64 [01:48<00:03,  1.75s/it, v_num=1, loss=1.650]Epoch 1:  97%|█████████▋| 62/64 [01:48<00:03,  1.75s/it, v_num=1, loss=1.210]Epoch 1:  98%|█████████▊| 63/64 [01:50<00:01,  1.75s/it, v_num=1, loss=1.210]Epoch 1:  98%|█████████▊| 63/64 [01:50<00:01,  1.75s/it, v_num=1, loss=1.590]Epoch 1: 100%|██████████| 64/64 [01:51<00:00,  1.75s/it, v_num=1, loss=1.590]Epoch 1: 100%|██████████| 64/64 [01:51<00:00,  1.75s/it, v_num=1, loss=1.520]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: 0it [00:00, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][AThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Validation DataLoader 0:  10%|█         | 1/10 [00:13<02:03, 13.68s/it][AThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Validation DataLoader 0:  20%|██        | 2/10 [00:27<01:50, 13.77s/it][AThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Validation DataLoader 0:  30%|███       | 3/10 [00:41<01:36, 13.81s/it][AThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Validation DataLoader 0:  40%|████      | 4/10 [00:54<01:21, 13.51s/it][AThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Validation DataLoader 0:  50%|█████     | 5/10 [01:06<01:06, 13.21s/it][AThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Validation DataLoader 0:  60%|██████    | 6/10 [01:18<00:52, 13.13s/it][AThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Validation DataLoader 0:  70%|███████   | 7/10 [01:33<00:39, 13.29s/it][AThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Validation DataLoader 0:  80%|████████  | 8/10 [01:47<00:26, 13.41s/it][AThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Validation DataLoader 0:  90%|█████████ | 9/10 [02:01<00:13, 13.53s/it][AThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Validation DataLoader 0: 100%|██████████| 10/10 [02:11<00:00, 13.13s/it][A{'testlen': 4808, 'reflen': 2621, 'guess': [4808, 4734, 4660, 4586], 'correct': [1492, 567, 287, 156]}
ratio: 1.834414345668892
                                                                             
                                                                        [A{'Bleu_1': 0.31031613976699035, 'Bleu_2': 0.19278781656378122, 'Bleu_3': 0.13179078651719464, 'Bleu_4': 0.09393694334091786, 'ROUGE_L': 0.3008613135042289, 'CIDEr': 0.017503454275237407}
Epoch 1: 100%|██████████| 64/64 [04:04<00:00,  3.82s/it, v_num=1, loss=1.520]
Validation DataLoader 0: 100%|██████████| 10/10 [02:11<00:00, 13.14s/it][A                                                                             
                                                                        [ASaving checkpoint at step 128 to /data/wyh21/Hint_R2GenGPT/save/mimic/v1/checkpoints/checkpoint_epoch1_step128_bleu0.093937_cider0.017503.pth.
Epoch 1: 100%|██████████| 64/64 [04:04<00:00,  3.83s/it, v_num=1, loss=1.520]
Validation DataLoader 0: 100%|██████████| 10/10 [02:11<00:00, 13.14s/it][A